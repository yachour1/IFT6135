{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTv0D26B9W2h"
   },
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdN5pnC8MMx-"
   },
   "source": [
    "This notebook is intended to produce the plots and figures for the report on Problem 1 of the practical. You should not run this notebook in Google Colab until you have finished constructing the correct solutions for transformer_solution.py and encoder_decoder_solution.py\n",
    "\n",
    "This notebook provides some limited commentary on several HuggingFace Features and toolage. You will use HuggingFace Datasets to load the Amazon Polarity dataset for sentiment analysis. The notebook will define a Bert tokenizer, collate functions, and then train and evaluate several models using the HuggingFace utilities mentioned above. Remember, the most crucial part here is running the experiments for the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount your Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link your assignment folder & install requirements\n",
    "Enter the path to the assignment folder in your Google Drive\n",
    "If you run this notebook locally or on a cluster (i.e. not on Google Colab)\n",
    "you can delete this cell which is specific to Google Colab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sj/7dm6k4h97vz4lt11vpzsdj5c0000gn/T/ipykernel_9270/1081942771.py:10: UserWarning: CUDA is not available.\n",
      "  warnings.warn('CUDA is not available.')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "# Check if CUDA is available\n",
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "  warnings.warn('CUDA is not available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "dt3NTvpsy4Oc",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Running on GPU\n",
    "For this assignment, it will be necessary to run your experiments on GPU. To make sure the notebook is running on GPU, you can change the notebook settings with\n",
    "* (EN) `Edit > Notebook Settings`\n",
    "* (FR) `Modifier > Paramètres du notebook`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RLVSmv9HoMH5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from sklearn.metrics import f1_score   \n",
    "import time\n",
    "\n",
    "from typing import List, Dict, Union, Optional, Tuple\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "from transformer import Transformer, MultiHeadedAttention\n",
    "from lstm import EncoderDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = load_dataset(\"yelp_polarity\", split=\"train\", cache_dir=\"assignment/data\")\n",
    "dataset_test = load_dataset(\"yelp_polarity\", split=\"test[:1000]\", cache_dir=\"assignment/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔍 Quick look at the data\n",
    "Lets have quick look at a few samples in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "title: Contrary to other reviews, I have zero complaints about the service or the prices. I have been getting tire service here for the past 5 years now, and compared to my experience with places like Pep Boys, these guys are experienced and know what they're doing. \\nAlso, this is one place that I do not feel like I am being taken advantage of, just because of my gender. Other auto mechanics have been notorious for capitalizing on my ignorance of cars, and have sucked my bank account dry. But here, my service and road coverage has all been well explained - and let up to me to decide. \\nAnd they just renovated the waiting room. It looks a lot better than it did in previous years.\n",
      "label: 1\n",
      "------------------------------\n",
      "title: Last summer I had an appointment to get new tires and had to wait a super long time. I also went in this week for them to fix a minor problem with a tire they put on. They \\\"\"fixed\\\"\" it for free, and the very next morning I had the same issue. I called to complain, and the \\\"\"manager\\\"\" didn't even apologize!!! So frustrated. Never going back.  They seem overpriced, too.\n",
      "label: 0\n",
      "------------------------------\n",
      "title: Friendly staff, same starbucks fair you get anywhere else.  Sometimes the lines can get long.\n",
      "label: 1\n"
     ]
    }
   ],
   "source": [
    "n_samples_to_see = 3\n",
    "for i in range(n_samples_to_see):\n",
    "  print(\"-\"*30)\n",
    "  print(\"title:\", dataset_test[i][\"text\"])\n",
    "  print(\"label:\", dataset_test[i][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzxwRDQFUtaG"
   },
   "source": [
    "### 1️. Tokenize the `text`\n",
    "Tokenize the `text`portion of each sample (i.e. parsing the text to smaller chunks). Tokenization can happen in many ways; traditionally, this was done based on the white spaces. With transformer-based models, tokenization is performed based on the frequency of occurrence of \"chunk of text\". This frequency can be learned in many different ways. However the most common one is the [**wordpiece**](https://arxiv.org/pdf/1609.08144v2.pdf) model. \n",
    "> The wordpiece model is generated using a data-driven approach to maximize the language-model likelihood\n",
    "of the training data, given an evolving word definition. Given a training corpus and a number of desired\n",
    "tokens $D$, the optimization problem is to select $D$ wordpieces such that the resulting corpus is minimal in the\n",
    "number of wordpieces when segmented according to the chosen wordpiece model.\n",
    "\n",
    "Under this model:\n",
    "1. Not all things can be converted to tokens depending on the model. For example, most models have been pretrained without any knowledge of emojis. So their token will be `[UNK]`, which stands for unknown.\n",
    "2. Some words will be mapped to multiple tokens!\n",
    "3. Depending on the kind of model, your tokens may or may not respect capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qCpNwaTYSo3U"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OMDqabyToBt",
    "outputId": "f3477698-69e0-41e6-c840-beca5796e5c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome',\n",
       " 'to',\n",
       " 'if',\n",
       " '##t',\n",
       " '##6',\n",
       " '##13',\n",
       " '##5',\n",
       " '.',\n",
       " 'we',\n",
       " 'now',\n",
       " 'teach',\n",
       " 'you',\n",
       " '[UNK]',\n",
       " '(',\n",
       " 'hugging',\n",
       " 'face',\n",
       " ')',\n",
       " 'library',\n",
       " ':',\n",
       " 'dd',\n",
       " '##d',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sample = \"Welcome to IFT6135. We now teach you 🤗(HUGGING FACE) Library :DDD.\"\n",
    "tokenizer.tokenize(input_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEu6aqReXqp6"
   },
   "source": [
    "### 2. Encoding\n",
    "Once we have tokenized the text, we then need to convert these chuncks to numbers so we can feed them to our model. This conversion is basically a look-up in a dictionary **from `str` $\\to$ `int`**. The tokenizer object can also perform this work. While it does so it will also add the *special* tokens needed by the model to the encodings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EpDGccrvYKnT",
    "outputId": "f5776da3-28fa-4fc2-d247-d231214395e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Token Encodings:\n",
      " [101, 6160, 2000, 2065, 2102, 2575, 17134, 2629, 1012, 2057, 2085, 6570, 2017, 100, 1006, 17662, 2227, 1007, 3075, 1024, 20315, 2094, 1012, 102]\n",
      "-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.\n",
      "--> Token Encodings Decoded:\n",
      " [CLS] welcome to ift6135. we now teach you [UNK] ( hugging face ) library : ddd. [SEP]\n"
     ]
    }
   ],
   "source": [
    "input_sample = \"Welcome to IFT6135. We now teach you 🤗(HUGGING FACE) Library :DDD.\" #@param {type: \"string\"}\n",
    "\n",
    "print(\"--> Token Encodings:\\n\",tokenizer.encode(input_sample))\n",
    "print(\"-.\"*15)\n",
    "print(\"--> Token Encodings Decoded:\\n\",tokenizer.decode(tokenizer.encode(input_sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DI8lFKZSZ2ZW"
   },
   "source": [
    "### 3️. Truncate/Pad samples\n",
    "Since all the sample in the batch will not have the same sequence length, we would need to truncate the longer sequences (i.e. the ones that exeed a predefined maximum length) and pad the shorter ones so we that we can equal length for all the samples in the batch. Once this is achieved, we would need to convert the result to `torch.Tensor`s and return. These tensors will then be retrieved from the [dataloader](https://https//pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    def __init__(self, tokenizer: str, max_len: int) -> None:\n",
    "        self.tokenizer_name = tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Union[str, int]]]) -> Dict[str, torch.Tensor]:\n",
    "        texts = list(map(lambda batch_instance: batch_instance[\"text\"], batch))\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\",\n",
    "            return_token_type_ids=False,\n",
    "        )\n",
    "        \n",
    "        labels = list(map(lambda batch_instance: int(batch_instance[\"label\"]), batch))\n",
    "        labels = torch.LongTensor(labels)\n",
    "        return dict(tokenized_inputs, **{\"labels\": labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🧑‍🍳 Setting up the collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "4VaSpuyIjNqn"
   },
   "outputs": [],
   "source": [
    "tokenizer_name = \"bert-base-uncased\"\n",
    "sample_max_length = 256\n",
    "collate = Collate(tokenizer=tokenizer_name, max_len=sample_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "y9P4oWyOSexA"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "class ReviewClassifier(nn.Module):\n",
    "    def __init__(self, backbone: str, backbone_hidden_size: int, nb_classes: int):\n",
    "        super(ReviewClassifier, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.backbone_hidden_size = backbone_hidden_size\n",
    "        self.nb_classes = nb_classes\n",
    "        self.back_bone = AutoModel.from_pretrained(\n",
    "            self.backbone,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "        self.classifier = torch.nn.Linear(self.backbone_hidden_size, self.nb_classes)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        back_bone_output = self.back_bone(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = back_bone_output[0]\n",
    "        pooled_output = hidden_states[:, 0]  # getting the [CLS] token\n",
    "        logits = self.classifier(pooled_output)\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, self.nb_classes),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "            return loss, logits\n",
    "        return logits\n",
    "\n",
    "class ReviewClassifierLSTM(nn.Module):\n",
    "    def __init__(self, nb_classes: int, encoder_only: bool = False, \n",
    "        with_attn: bool = True, dropout: int = 0.5, hidden_size: int = 256):\n",
    "        super(ReviewClassifierLSTM, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "        self.encoder_only = encoder_only\n",
    "\n",
    "        if with_attn:\n",
    "            attn = MultiHeadedAttention(head_size = 2*hidden_size, num_heads=1)\n",
    "        else:\n",
    "            attn = None\n",
    "            \n",
    "        self.back_bone = EncoderDecoder(dropout=dropout, encoder_only=encoder_only,\n",
    "                                        attn=attn, hidden_size=hidden_size)\n",
    "        \n",
    "        if self.encoder_only:\n",
    "            self.classifier = torch.nn.Linear(hidden_size*2, self.nb_classes)\n",
    "        else:\n",
    "            self.classifier = torch.nn.Linear(hidden_size, self.nb_classes)\n",
    "       \n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        pooled_output, _ = self.back_bone(input_ids, attention_mask)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, self.nb_classes),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "            return loss, logits\n",
    "        return logits\n",
    "\n",
    "\n",
    "class ReviewClassifierTransformer(nn.Module):\n",
    "    def __init__(self, nb_classes: int, num_heads: int = 4, num_layers: int = 4, block: str=\"prenorm\", dropout: float = 0.3):\n",
    "        super(ReviewClassifierTransformer, self).__init__()\n",
    "        self.nb_classes = nb_classes\n",
    "        self.back_bone = Transformer(num_heads=num_heads, num_layers=num_layers, block=block, dropout=dropout)\n",
    "        self.classifier = torch.nn.Linear(256, self.nb_classes)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
    "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
    "        attention_mask = torch.cat([torch.ones(attention_mask.shape[0]).unsqueeze(1).to(device),\n",
    "                                    attention_mask], dim=1)\n",
    "        back_bone_output = self.back_bone(input_ids, attention_mask)\n",
    "        hidden_states = back_bone_output\n",
    "        pooled_output = hidden_states\n",
    "        logits = self.classifier(pooled_output)\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(\n",
    "                logits.view(-1, self.nb_classes),\n",
    "                labels.view(-1),\n",
    "            )\n",
    "            return loss, logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "HP58LrWUjFt4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Device selected: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"--> Device selected: {device}\")\n",
    "def train_one_epoch(\n",
    "    model: torch.nn.Module, training_data_loader: DataLoader, optimizer: torch.optim.Optimizer, logging_frequency: int, testing_data_loader: DataLoader, logger: dict):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    epoch_loss = 0\n",
    "    logging_loss = 0\n",
    "    start_time = time.time()\n",
    "    mini_start_time = time.time()\n",
    "    for step, batch in enumerate(training_data_loader):\n",
    "        batch = {key: value.to(device) for key, value in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        logging_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % logging_frequency == 0:\n",
    "            freq_time = time.time()-mini_start_time\n",
    "            logger['train_time'].append(freq_time+logger['train_time'][-1])\n",
    "            logger['train_losses'].append(logging_loss/logging_frequency)\n",
    "            print(f\"Training loss @ step {step+1}: {logging_loss/logging_frequency}\")\n",
    "            eval_acc, eval_f1, eval_loss, eval_time = evaluate(model, testing_data_loader)\n",
    "            logger['eval_accs'].append(eval_acc)\n",
    "            logger['eval_f1s'].append(eval_f1)\n",
    "            logger['eval_losses'].append(eval_loss)\n",
    "            logger['eval_time'].append(eval_time+logger['eval_time'][-1])\n",
    "\n",
    "            logging_loss = 0\n",
    "            mini_start_time = time.time()\n",
    "\n",
    "    return epoch_loss / len(training_data_loader), time.time()-start_time\n",
    "\n",
    "\n",
    "def evaluate(model: torch.nn.Module, test_data_loader: DataLoader):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    eval_loss = 0\n",
    "    correct_predictions = {i: 0 for i in range(2)}\n",
    "    total_predictions = {i: 0 for i in range(2)}\n",
    "    preds = []\n",
    "    targets = []\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(test_data_loader):\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs[0]\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            predictions = np.argmax(outputs[1].detach().cpu().numpy(), axis=1)\n",
    "            preds.extend(predictions.tolist())\n",
    "            targets.extend(batch[\"labels\"].cpu().numpy().tolist())\n",
    "\n",
    "            for target, prediction in zip(batch[\"labels\"].cpu().numpy(), predictions):\n",
    "                if target == prediction:\n",
    "                    correct_predictions[target] += 1\n",
    "                total_predictions[target] += 1\n",
    "    accuracy = (100.0 * sum(correct_predictions.values())) / sum(total_predictions.values())\n",
    "    f1 = f1_score(targets, preds)\n",
    "    model.train()\n",
    "    return accuracy, round(f1, 4), eval_loss / len(test_data_loader), time.time() - start_time\n",
    "\n",
    "\n",
    "def save_logs(dictionary, log_dir, exp_id):\n",
    "  log_dir = os.path.join(log_dir, exp_id)\n",
    "  os.makedirs(log_dir, exist_ok=True)\n",
    "  # Log arguments\n",
    "  with open(os.path.join(log_dir, \"args.json\"), \"w\") as f:\n",
    "    json.dump(dictionary, f, indent=2)\n",
    "\n",
    "def save_model(model, log_dir, exp_id):\n",
    "  log_dir = os.path.join(log_dir, exp_id)\n",
    "  os.makedirs(log_dir, exist_ok=True)\n",
    "  # Save model\n",
    "  torch.save(model.state_dict(), f\"assignment/models/model_{exp_id}.pt\")\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Problem 3\n",
    "Feel free to modify this code however it is convenient for you to produce a report except for the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting 1: LSTM, no dropout, encoder only\n",
      "Epoch 1\n",
      "Training loss @ step 100: 0.6899231749773026\n",
      "Training loss @ step 200: 0.6839810907840729\n",
      "Training loss @ step 300: 0.6725757032632828\n",
      "Training loss @ step 400: 0.6573752403259278\n",
      "Training loss @ step 500: 0.633895851969719\n"
     ]
    }
   ],
   "source": [
    "logging_frequency = 100\n",
    "learning_rate = 1e-5\n",
    "nb_epoch=2\n",
    "\n",
    "for i in range(1, 3):\n",
    "  experimental_setting = i\n",
    "\n",
    "  if experimental_setting == 1:\n",
    "    print(\"Setting 1: LSTM, no dropout, encoder only\")\n",
    "    model = ReviewClassifierLSTM(nb_classes=2, dropout=0, encoder_only=True)\n",
    "  if experimental_setting == 2:\n",
    "    print(\"Setting 2: LSTM, dropout, encoder only\")\n",
    "    model = ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=True)\n",
    "  if experimental_setting == 3:\n",
    "    print(\"Setting 3: LSTM, dropout, encoder-decoder, no attention\")\n",
    "    model = ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=False, with_attn=False)\n",
    "  if experimental_setting == 4:\n",
    "    print(\"Setting 4: LSTM, dropout, encoder-decoder, with attention\")\n",
    "    model = ReviewClassifierLSTM(nb_classes=2, dropout=0.3, encoder_only=False, with_attn=True)\n",
    "  if experimental_setting == 5:\n",
    "    print(\"Setting 5: Transformer, 2 layers, pre-normalization\")\n",
    "    model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=2, block='prenorm', dropout=0.3)\n",
    "  if experimental_setting == 6:\n",
    "    print(\"Setting 6: Transformer, 4 layers, pre-normalization\")\n",
    "    model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=4, block='prenorm', dropout=0.3)\n",
    "  if experimental_setting == 7:\n",
    "    print(\"Setting 7: Transformer, 2 layers, post-normalization\")\n",
    "    model = ReviewClassifierTransformer(nb_classes=2, num_heads=4, num_layers=2, block='postnorm', dropout=0.3)\n",
    "  if experimental_setting == 8:\n",
    "    nb_epoch = 2\n",
    "    print(\"Setting 8: Fine-tuning BERT\")\n",
    "    model = ReviewClassifier(backbone=\"bert-base-uncased\", backbone_hidden_size=768, nb_classes=2)\n",
    "    for parameter in model.back_bone.parameters():\n",
    "      parameter.requires_grad= False\n",
    "\n",
    "\n",
    "  # setting up the optimizer\n",
    "  optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, eps=1e-8)\n",
    "  model.to(device)\n",
    "  logger = dict()\n",
    "  logger['train_time'] = [0]\n",
    "  logger['eval_time'] = [0]\n",
    "  logger['train_losses'] = []\n",
    "  logger['eval_accs'] = []\n",
    "  logger['eval_f1s'] = []\n",
    "  logger['eval_losses'] = []\n",
    "  logger[\"epoch_train_loss\"] = []\n",
    "  logger[\"epoch_train_time\"] = []\n",
    "  logger[\"epoch_eval_loss\"] = []\n",
    "  logger[\"epoch_eval_time\"] = []\n",
    "  logger[\"epoch_eval_acc\"] = []\n",
    "  logger[\"epoch_eval_f1\"] = []\n",
    "  \n",
    "  logger['parameters'] = sum([p.numel() for p in model.back_bone.parameters() if p.requires_grad])\n",
    "\n",
    "  for epoch in range(nb_epoch):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    if experimental_setting == 8 and epoch>1: #unfreezing layer 10 for fine-tuning\n",
    "      for name, param in model.back_bone.named_parameters():\n",
    "        if name.startswith(\"encoder.layer.11\"):\n",
    "            param.requires_grad = True\n",
    "    train_loss, train_time = train_one_epoch(model, train_loader, optimizer, logging_frequency, test_loader, logger)\n",
    "    eval_acc, eval_f1, eval_loss, eval_time  = evaluate(model, test_loader)\n",
    "    logger[\"epoch_train_loss\"].append(train_loss)\n",
    "    logger[\"epoch_train_time\"].append(train_time)\n",
    "    logger[\"epoch_eval_loss\"].append(eval_loss)\n",
    "    logger[\"epoch_eval_time\"].append(eval_time)\n",
    "    logger[\"epoch_eval_acc\"].append(eval_acc)\n",
    "    logger[\"epoch_eval_f1\"].append(eval_f1)\n",
    "    print(f\"    Epoch: {epoch+1} Loss/Test: {eval_loss}, Loss/Train: {train_loss}, Acc/Test: {eval_acc}, F1/Test: {eval_f1}, Train Time: {train_time}, Eval Time: {eval_time}\")\n",
    "  \n",
    "  logger['train_time'] = logger['train_time'][1:]\n",
    "  logger['eval_time'] = logger['eval_time'][1:]\n",
    "  save_logs(logger, \"log\", str(experimental_setting))\n",
    "  save_model(model, \"models\", str(experimental_setting))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the path to the log files\n",
    "log_path = 'log'\n",
    "\n",
    "# Define the number of experiments\n",
    "num_exps = 8\n",
    "\n",
    "# Loop through each experiment\n",
    "for i in range(1, num_exps+1):\n",
    "\n",
    "  # Load the log file for the current experiment\n",
    "  log_file = f'{log_path}/log_{i}.pkl'\n",
    "  with open(log_file, 'rb') as f:\n",
    "    log = pickle.load(f)\n",
    "\n",
    "  # Extract the training and validation loss and accuracy data\n",
    "  train_loss = log['epoch_train_loss']\n",
    "  val_loss = log['epoch_eval_loss']\n",
    "  val_acc = log['epoch_eval_acc']\n",
    "\n",
    "  # Create a figure and axis object with labeled axes and legend\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.set_xlabel('Learning Iterations')\n",
    "  ax.set_ylabel('Loss/Accuracy')\n",
    "  ax.legend(['Train Loss', 'Validation Loss', 'Validation Accuracy'])\n",
    "\n",
    "  # Plot the learning curves\n",
    "  ax.plot(train_loss, label='Train Loss')\n",
    "  ax.plot(val_loss, label='Validation Loss')\n",
    "  ax.plot(val_acc, label='Validation Accuracy')\n",
    "\n",
    "  # Add an explanatory legend\n",
    "  plt.title(f'Experiment {i}: Learning Curves')\n",
    "\n",
    "  # Show the plot\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Augment the original reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.augmentation import Augmenter\n",
    "from textattack.transformations import WordSwapQWERTY\n",
    "from textattack.transformations import WordSwapExtend\n",
    "from textattack.transformations import WordSwapContract\n",
    "from textattack.transformations import WordSwapHomoglyphSwap\n",
    "from textattack.transformations import CompositeTransformation\n",
    "from textattack.transformations import WordSwapRandomCharacterDeletion\n",
    "from textattack.transformations import WordSwapNeighboringCharacterSwap\n",
    "from textattack.transformations import WordSwapRandomCharacterInsertion\n",
    "from textattack.transformations import WordSwapRandomCharacterSubstitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word-level Augmentations\n",
    "word_swap_contract = True\n",
    "word_swap_extend = False\n",
    "word_swap_homoglyph_swap = False\n",
    "\n",
    "\n",
    "# Character-level Augmentations\n",
    "word_swap_neighboring_character_swap = True\n",
    "word_swap_qwerty = False\n",
    "word_swap_random_character_deletion = False\n",
    "word_swap_random_character_insertion = False\n",
    "word_swap_random_character_substitution = False\n",
    "\n",
    "# Check all the augmentations that you wish to apply!\n",
    "\n",
    "# NOTE: Try applying each augmentation individually, and observe the changes.\n",
    "\n",
    "# Apply augmentations\n",
    "augmentations = []\n",
    "if word_swap_contract:\n",
    "  augmentations.append(WordSwapContract())\n",
    "if word_swap_extend:\n",
    "  augmentations.append(WordSwapExtend())\n",
    "if word_swap_homoglyph_swap:\n",
    "  augmentations.append(WordSwapHomoglyphSwap())\n",
    "if word_swap_neighboring_character_swap:\n",
    "  augmentations.append(WordSwapNeighboringCharacterSwap())\n",
    "if word_swap_qwerty:\n",
    "  augmentations.append(WordSwapQWERTY())\n",
    "if word_swap_random_character_deletion:\n",
    "  augmentations.append(WordSwapRandomCharacterDeletion())\n",
    "if word_swap_random_character_insertion:\n",
    "  augmentations.append(WordSwapRandomCharacterInsertion())\n",
    "if word_swap_random_character_substitution:\n",
    "  augmentations.append(WordSwapRandomCharacterSubstitution())\n",
    "\n",
    "transformation = CompositeTransformation(augmentations)\n",
    "augmenter = Augmenter(transformation=transformation,\n",
    "                      transformations_per_example=1)\n",
    "\n",
    "\n",
    "review = \"I loved the food and the service was great!\"\n",
    "augmented_review = augmenter.augment(review)[0]\n",
    "print(\"Augmented review:\\n\")\n",
    "print(augmented_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPrediction(text):\n",
    "  \"\"\"\n",
    "  Outputs model prediction based on the input text.\n",
    "\n",
    "  Args:\n",
    "    text: String\n",
    "      Input text\n",
    "\n",
    "  Returns:\n",
    "    item of pred: Iterable\n",
    "      Prediction on the input text\n",
    "  \"\"\"\n",
    "  inputs = tokenizer(text, padding=\"max_length\", max_length=256,\n",
    "                     truncation=True, return_tensors=\"pt\")\n",
    "  for key, value in inputs.items():\n",
    "    inputs[key] = value.to(model.device)\n",
    "\n",
    "\n",
    "  outputs = model(**inputs)\n",
    "  logits = outputs.logits\n",
    "  pred = torch.argmax(logits, dim=1)\n",
    "  return pred.item()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
